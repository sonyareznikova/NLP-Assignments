{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import shelve\n",
    "import copy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/Sonya/anaconda/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with shelve.open('index','c') as index:\n",
    "    index.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = shelve.open('index', writeback = True)\n",
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [1, 2, 3, 120]\n",
      "z [7, 8, 9, 120]\n",
      "y [4, 5, 6, 120]\n",
      "w [10, 11, 12, 120]\n"
     ]
    }
   ],
   "source": [
    "with shelve.open('example', 'c') as example:\n",
    "    example['x'] = [1,2,3]\n",
    "    example['y'] = [4,5,6]\n",
    "    example['z'] = [7,8,9]\n",
    "    example['w'] = [10,11,12]\n",
    "    example.sync()\n",
    "\n",
    "ex = shelve.open('example', writeback=True)\n",
    "for i in ex:\n",
    "    ex[i].append(120)\n",
    "    ex.sync()\n",
    "for i in ex:\n",
    "    print(i, ex[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "len(gutenberg.fileids())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unigrams\n",
    "unwanted = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "unwanted.update(list('!\"#$%&\\'()*+,-./:;<=>? @[\\\\]`^_`{|}~£'))\n",
    "unwanted.update([\"--\", \"'s\", \"mr\", \"mrs\", \"''\", \"``\", \"\", \"'t\",])\n",
    "\n",
    "def get_rid_of_underscores_and_dots_and_upper(word):\n",
    "    if word[0]==\"_\" and word[-1]==\"_\":\n",
    "        return word[1:-1].lower()\n",
    "    elif word[-1]==\".\":\n",
    "        return word[:-1].lower()\n",
    "    elif word[0]==\"`\":\n",
    "        return word[1:]\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "def tokenise_and_clean(corpus):\n",
    "    doc_to_tokens = {}\n",
    "    for document_id in corpus.fileids():\n",
    "        tokens = nltk.word_tokenize(corpus.raw(document_id))\n",
    "        clean_tokens = [get_rid_of_underscores_and_dots_and_upper(t) for t in tokens]\n",
    "        clean_tokens = filter(lambda x: x not in unwanted, clean_tokens)\n",
    "        doc_to_tokens[document_id] = list(clean_tokens)\n",
    "    return doc_to_tokens\n",
    "\n",
    "docs_to_tokens = tokenise_and_clean(gutenberg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bigrams\n",
    "\n",
    "def bigramise_and_clean(corpus):\n",
    "    doc_to_bigrams = {}\n",
    "    for document_id in corpus.fileids():\n",
    "        words = corpus.words(document_id)\n",
    "        clean_words = [get_rid_of_underscores_and_dots_and_upper(w) for w in words]\n",
    "        clean_words = filter(lambda x: x not in unwanted, clean_words)\n",
    "        doc_to_bigrams[document_id] = list(nltk.bigrams(clean_words))\n",
    "    return doc_to_bigrams\n",
    "\n",
    "docs_to_bigrams = bigramise_and_clean(gutenberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emma', 'jane'),\n",
       " ('jane', 'austen'),\n",
       " ('austen', '1816'),\n",
       " ('1816', 'volume'),\n",
       " ('volume', 'chapter'),\n",
       " ('chapter', 'emma'),\n",
       " ('emma', 'woodhouse'),\n",
       " ('woodhouse', 'handsome'),\n",
       " ('handsome', 'clever'),\n",
       " ('clever', 'rich')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_to_bigrams[\"austen-emma.txt\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00,  9.01it/s]\n",
      "  6%|▌         | 1/18 [00:00<00:02,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('austen-emma.txt', 0.0009302456694245162), ('austen-persuasion.txt', 0.0007447402718301992), ('austen-sense.txt', 0.0006775705332103669), ('blake-poems.txt', 0.002103049421661409), ('bryant-stories.txt', 0.0013802622498274672), ('burgess-busterbrown.txt', 0.0008984725965858042), ('carroll-alice.txt', 7.439369141496801e-05), ('chesterton-ball.txt', 0.00025145213608589607), ('chesterton-brown.txt', 0.0004815318377520961), ('chesterton-thursday.txt', 0.00017682215227923754), ('edgeworth-parents.txt', 0.0006108890981749688), ('melville-moby_dick.txt', 0.0002664021018207206), ('shakespeare-hamlet.txt', 0.00012543116964565694), ('shakespeare-macbeth.txt', 0.0002947244326554671), ('whitman-leaves.txt', 1.5311121999020088e-05)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:04<00:00,  5.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "index_dictionary = {}\n",
    "index_bigrams = {}\n",
    "def index_files(index_filename, documents_to_tokens):\n",
    "    for document_id in tqdm(documents_to_tokens.keys()):\n",
    "        frequency_of_all_tokens = Counter(documents_to_tokens[document_id])\n",
    "        document_len = len(documents_to_tokens[document_id])\n",
    "        for token in documents_to_tokens[document_id]:\n",
    "            term_frequency_per_document = (frequency_of_all_tokens[token])/(document_len)\n",
    "            if token not in index_filename:\n",
    "                index_filename[token] = []\n",
    "            if (document_id, term_frequency_per_document) not in index_filename[token]:    \n",
    "                index_filename[token].append((document_id, term_frequency_per_document))\n",
    "\n",
    "index_files(index_dictionary, docs_to_tokens)\n",
    "print(index_dictionary['pretty'])\n",
    "index_files(index_bigrams, docs_to_bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_documents = len(gutenberg.fileids())\n",
    "#index in form {'keyword': (document_id, term_frequency)} and n_documents as number of all documents in the corpus\n",
    "def rank_tf_idf(index, n_document):\n",
    "    ranked_index = {}\n",
    "    for word in index:\n",
    "        docs_with_word = len(index[word])\n",
    "        ranked_index[word] = []\n",
    "        ranks = [] #for sorting\n",
    "        for pair in index[word]:\n",
    "            doc_id = pair[0]\n",
    "            doc_freq = 1+math.log(n_document/docs_with_word, 2)\n",
    "            rank = (doc_id, doc_freq*pair[1])\n",
    "            ranks.append(rank)\n",
    "        ranked_index[word].append(sorted(ranks, key=operator.itemgetter(1), reverse=True))\n",
    "    return ranked_index\n",
    "\n",
    "ranked = rank_tf_idf(index_dictionary, len(gutenberg.fileids()))\n",
    "ranked_bigrams = rank_tf_idf(index_bigrams, len(gutenberg.fileids()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('austen-sense.txt', 0.00032264985156341965),\n",
       "  ('austen-persuasion.txt', 0.0002779184325330371),\n",
       "  ('austen-emma.txt', 9.528646042902362e-05)]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_bigrams[('pretty', 'girl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes query_tfidfs as {'keyword': tfidf} and doc_per_word as {'keyword': tfidf} in one document\n",
    "def cosine_similarity(query_tfidfs, tfidf_per_word):\n",
    "    query_sum = 0\n",
    "    doc_sum = 0\n",
    "    dot_product = 0\n",
    "    for query_word in query_tfidfs:\n",
    "        query_sum += (query_tfidfs[query_word]**2)\n",
    "        doc_sum += (tfidf_per_word[query_word]**2)\n",
    "        dot_product += (query_tfidfs[query_word] * tfidf_per_word[query_word])\n",
    "    query_norm = math.sqrt(query_sum)\n",
    "    doc_norm = math.sqrt(doc_sum)\n",
    "    if query_norm * doc_norm == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product/(query_norm * doc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf_for_document_id(document_id, subset_from_index):\n",
    "    tfidfs_per_word = {}\n",
    "    for word in subset_from_index:\n",
    "        for pair in subset_from_index[word]:\n",
    "            if pair[0] == document_id:\n",
    "                tfidfs_per_word[word] = pair[1]\n",
    "        #in case some query words are not present in the document\n",
    "        if word not in tfidfs_per_word:\n",
    "            tfidfs_per_word[word] = 0\n",
    "    \n",
    "    return tfidfs_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes result as a tuple of unigram_index search and bigram_index search, both as [(document_id, rank), ..]\n",
    "def find_example(results, query_unigrams, query_bigrams, corpus):\n",
    "    substring_window = 20 #to show the query with this many symbols on both sides\n",
    "    top_uni = results[0]\n",
    "    top_bi = results[1]\n",
    "    result_counter = 0 #number of results already shown\n",
    "    printed_docs = [] #to store docs we've already shown\n",
    "    \n",
    "    for pair in top_bi:\n",
    "        if pair[1] > 0:\n",
    "            print(pair[0].upper())\n",
    "            result_counter += 1\n",
    "            printed_docs.append(pair[0])\n",
    "            for bigram in query_bigrams:\n",
    "                try:\n",
    "                    bg = \" \".join(list(bigram))\n",
    "                    occurence_index = corpus.raw(pair[0]).lower().index(bg)\n",
    "                    if occurence_index < substring_window:\n",
    "                        print(corpus.raw(pair[0])[:occurence_index+substring_window]+\"...\\n\")\n",
    "                    else:\n",
    "                        print(\"...\"+corpus.raw(pair[0])[occurence_index-substring_window:occurence_index+substring_window]+\"...\\n\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    #if there aren't enough results, we can go to the unigram index and supply more documents with parts of the query\n",
    "    # '4' is an arbitrary number, seemed most suitable\n",
    "    if result_counter < 4:\n",
    "        for pair in top_uni:\n",
    "            if pair[1] > 0 and pair[0] not in printed_docs and result_counter < 8:\n",
    "                print(pair[0].upper())\n",
    "                result_counter += 1\n",
    "                printed_docs.append(pair[0])\n",
    "                for unigram in query_unigrams:\n",
    "                    try:\n",
    "                        occurence_index = corpus.raw(pair[0]).lower().index(unigram)\n",
    "                        if occurence_index < substring_window:\n",
    "                            print(corpus.raw(pair[0])[:occurence_index+substring_window]+\"...\\n\")\n",
    "                        else:\n",
    "                            print(\"...\"+corpus.raw(pair[0])[occurence_index-substring_window:occurence_index+substring_window]+\"...\\n\")\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to search for?\n",
      "Moby Dick is a great whale with fangs\n",
      "([('melville-moby_dick.txt', 0.3941763281569343), ('blake-poems.txt', 0.29036753123472075), ('bryant-stories.txt', 0.1583008477092572), ('shakespeare-hamlet.txt', 0.14707595038229476), ('austen-persuasion.txt', 0.13549352295722214), ('chesterton-ball.txt', 0.12412064344595966), ('whitman-leaves.txt', 0.11811958663761601), ('bible-kjv.txt', 0.11719189749544677), ('shakespeare-macbeth.txt', 0.11625530846729783), ('austen-emma.txt', 0.11625530846729781), ('carroll-alice.txt', 0.11625530846729781), ('chesterton-brown.txt', 0.11625530846729781), ('edgeworth-parents.txt', 0.11625530846729781), ('shakespeare-caesar.txt', 0.11625530846729781), ('austen-sense.txt', 0.1162553084672978), ('burgess-busterbrown.txt', 0.1162553084672978), ('chesterton-thursday.txt', 0.1162553084672978), ('milton-paradise.txt', 0.1162553084672978)], [('melville-moby_dick.txt', 0.7784130394753296), ('austen-emma.txt', 0), ('austen-persuasion.txt', 0), ('austen-sense.txt', 0), ('bible-kjv.txt', 0), ('blake-poems.txt', 0), ('bryant-stories.txt', 0), ('burgess-busterbrown.txt', 0), ('carroll-alice.txt', 0), ('chesterton-ball.txt', 0), ('chesterton-brown.txt', 0), ('chesterton-thursday.txt', 0), ('edgeworth-parents.txt', 0), ('milton-paradise.txt', 0), ('shakespeare-caesar.txt', 0), ('shakespeare-hamlet.txt', 0), ('shakespeare-macbeth.txt', 0), ('whitman-leaves.txt', 0)])\n",
      "MELVILLE-MOBY_DICK.TXT\n",
      "[Moby Dick by Herman ...\n",
      "\n",
      "...\n",
      "\n",
      "\"And God created great whales.\" --GEN...\n",
      "\n",
      "BLAKE-POEMS.TXT\n",
      "...usands of sweepers, Dick, Joe, Ned, and ...\n",
      "\n",
      "...ble to thought\n",
      "   A greater than itself ...\n",
      "\n",
      "BRYANT-STORIES.TXT\n",
      "...e of earth was ever greater in her heart...\n",
      "\n",
      "...R RABBIT FOOLED THE WHALE AND THE ELEPHA...\n",
      "\n",
      "SHAKESPEARE-HAMLET.TXT\n",
      "...kes to day,\n",
      "But the great Cannon to the ...\n",
      "\n",
      "...\n",
      "\n",
      "   Ham. Or like a Whale?\n",
      "  Polon. Veri...\n",
      "\n",
      "AUSTEN-PERSUASION.TXT\n",
      "...eling, unprofitable Dick Musgrove, who h...\n",
      "\n",
      "...alter Elliot, Esq., great grandson of\n",
      "th...\n",
      "\n",
      "CHESTERTON-BALL.TXT\n",
      "...e had not even very greatly improved, th...\n",
      "\n",
      "...to believe that the whale swallowed Jona...\n",
      "\n",
      "WHITMAN-LEAVES.TXT\n",
      "...ng to every port to dicker and adventure...\n",
      "\n",
      "...r, and a longer and greater one than any...\n",
      "\n",
      "...tands braced in the whale-boat, lance an...\n",
      "\n",
      "BIBLE-KJV.TXT\n",
      "...16 And God made two great lights; the gr...\n",
      "\n",
      "...d God created great whales, and every li...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = gutenberg\n",
    "def search(query, corpus):\n",
    "    #dividing query into unigrams and bigrams\n",
    "    q_tokens = nltk.word_tokenize(query)\n",
    "    clean_q_tokens = [get_rid_of_underscores_and_dots_and_upper(t) for t in q_tokens]\n",
    "    bg_clean_q_tokens = clean_q_tokens\n",
    "    \n",
    "    clean_q_tokens = filter(lambda x: x not in unwanted, clean_q_tokens)\n",
    "    bg_clean_q_tokens = filter(lambda x: x not in unwanted, bg_clean_q_tokens)\n",
    "    \n",
    "    query_bigrams = list(nltk.bigrams(bg_clean_q_tokens))\n",
    "    \n",
    "    list_of_tokens = list(clean_q_tokens)\n",
    "    query_length = len(list_of_tokens)\n",
    "    \n",
    "    query_bg_length = len(query_bigrams)\n",
    "    \n",
    "    #calculating tf-idfs for query terms in unigrams\n",
    "    frequencies = Counter(list_of_tokens)\n",
    "    frequency_of_query_terms = {}\n",
    "    ranked_docs_per_term = {}\n",
    "    for term in frequencies:\n",
    "        if term in index_dictionary:\n",
    "            frequency_of_query_terms[term] = frequencies[term]/query_length\n",
    "            term_idf = 1+math.log(n_documents/len(index_dictionary[term]), 2)\n",
    "            frequency_of_query_terms[term] *= term_idf\n",
    "            ranked_docs_per_term[term] = index_dictionary[term]\n",
    "            \n",
    "    #calculating tf-idfs for query bigrams\n",
    "    bg_frequencies = Counter(query_bigrams)\n",
    "    bg_frequency_of_query_terms = {}\n",
    "    bg_ranked_docs_per_term = {}\n",
    "    for term in bg_frequencies:\n",
    "        if term in index_bigrams:\n",
    "            bg_frequency_of_query_terms[term] = bg_frequencies[term]/query_bg_length\n",
    "            term_idf = 1+math.log(n_documents/len(index_bigrams[term]), 2)\n",
    "            bg_frequency_of_query_terms[term] *= term_idf\n",
    "            bg_ranked_docs_per_term[term] = index_bigrams[term]\n",
    "    \n",
    "    #if no query words exist in our index\n",
    "    if len(ranked_docs_per_term) == 0 and len(bg_ranked_docs_per_term) == 0:\n",
    "        print(\"Please try another key phrase, we can't seem to find this one, sorry...\\n\")\n",
    "        return\n",
    "    \n",
    "    #comparing similarities of query and docs vectors\n",
    "    ranked_docs_per_query = {}\n",
    "    bg_ranked_docs_per_query = {}\n",
    "    for document_id in corpus.fileids():\n",
    "        tfidfs_per_word = get_tfidf_for_document_id(document_id, ranked_docs_per_term)\n",
    "        bg_tfidfs_per_word = get_tfidf_for_document_id(document_id, bg_ranked_docs_per_term)\n",
    "        cs = cosine_similarity(frequency_of_query_terms, tfidfs_per_word)\n",
    "        bg_cs = cosine_similarity(bg_frequency_of_query_terms, bg_tfidfs_per_word)\n",
    "        if cs != 0 or bg_cs != 0:\n",
    "            ranked_docs_per_query[document_id] = cs\n",
    "            bg_ranked_docs_per_query[document_id] = bg_cs\n",
    "    \n",
    "    sorted_ranked_docs = sorted(ranked_docs_per_query.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    bg_sorted_ranked_docs = sorted(bg_ranked_docs_per_query.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    results = (sorted_ranked_docs, bg_sorted_ranked_docs)\n",
    "    print(results)\n",
    "    find_example(results, list_of_tokens, query_bigrams, corpus)\n",
    "    \n",
    "query = input(\"What would you like to search for?\\n\")        \n",
    "results = search(query, gutenberg)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Moby Dick by Herman Melville 1'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = gutenberg.raw('melville-moby_dick.txt').index('Moby Dick')\n",
    "gutenberg.raw('melville-moby_dick.txt')[:ind+30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def search_old(keywords):\\n    k_tokens = nltk.word_tokenize(keywords)\\n    clean_k_tokens = [get_rid_of_underscores_and_dots_and_upper(t) for t in k_tokens]\\n    clean_k_tokens = filter(lambda x: x not in unwanted, clean_k_tokens)\\n    \\n    docs_per_word = {}\\n    only_docs_per_word = []\\n    for word in list(clean_k_tokens):\\n        if word in index_dictionary:\\n            docs_per_word[word] = index_dictionary[word] #get document\\'s name and tf-idf\\n            this_words_docs = map(operator.itemgetter(0), index_dictionary[word]) #get only document\\'s name\\n            only_docs_per_word.append(set(this_words_docs)) \\n    common_docs = set.intersection(*only_docs_per_word)\\n    \\n    all_docs = [item for sublist in docs_per_word.values() for item in sublist]\\n    final_doclist_for_query = []\\n    for item in all_docs:\\n        if item[0] in common_docs:\\n            final_doclist_for_query.append(item)\\n    return final_doclist_for_query\\n\\nsearch(\"crazy baby\")'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def search_old(keywords):\n",
    "    k_tokens = nltk.word_tokenize(keywords)\n",
    "    clean_k_tokens = [get_rid_of_underscores_and_dots_and_upper(t) for t in k_tokens]\n",
    "    clean_k_tokens = filter(lambda x: x not in unwanted, clean_k_tokens)\n",
    "    \n",
    "    docs_per_word = {}\n",
    "    only_docs_per_word = []\n",
    "    for word in list(clean_k_tokens):\n",
    "        if word in index_dictionary:\n",
    "            docs_per_word[word] = index_dictionary[word] #get document's name and tf-idf\n",
    "            this_words_docs = map(operator.itemgetter(0), index_dictionary[word]) #get only document's name\n",
    "            only_docs_per_word.append(set(this_words_docs)) \n",
    "    common_docs = set.intersection(*only_docs_per_word)\n",
    "    \n",
    "    all_docs = [item for sublist in docs_per_word.values() for item in sublist]\n",
    "    final_doclist_for_query = []\n",
    "    for item in all_docs:\n",
    "        if item[0] in common_docs:\n",
    "            final_doclist_for_query.append(item)\n",
    "    return final_doclist_for_query\n",
    "\n",
    "search(\"crazy baby\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
