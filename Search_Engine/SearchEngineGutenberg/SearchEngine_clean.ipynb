{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells that take some time to run (cells for the indexing and ranking) are commented out in case you decide to run all the cells at once to test the engine. The search query input is the last cell of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import copy\n",
    "import shelve\n",
    "from nltk.corpus import gutenberg\n",
    "from collections import Counter\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global variables\n",
    "#unwanted = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "unwanted = set()\n",
    "unwanted.update(list('!\"#$%&\\'()*+,-./:;<=>? @[\\\\]`^_`{|}~£'))\n",
    "unwanted.update([\"--\", \"'s\", \"mr\", \"mrs\", \"''\", \"``\", \"\", \"'t\",])\n",
    "\n",
    "corpus = gutenberg\n",
    "\n",
    "n_documents = len(gutenberg.fileids()) #number of documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#method for cleaning the data\n",
    "def get_rid_of_underscores_and_dots_and_upper(word):\n",
    "    if word[0]==\"_\" and word[-1]==\"_\":\n",
    "        return word[1:-1].lower()\n",
    "    elif word[-1]==\".\":\n",
    "        return word[:-1].lower()\n",
    "    elif word[0]==\"`\":\n",
    "        return word[1:]\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "#dividing into tokens - unigrams\n",
    "def tokenise_and_clean(corpus):\n",
    "    doc_to_tokens = {}\n",
    "    for document_id in tqdm(corpus.fileids()):\n",
    "        tokens = nltk.word_tokenize(corpus.raw(document_id))\n",
    "        clean_tokens = [get_rid_of_underscores_and_dots_and_upper(t) for t in tokens]\n",
    "        clean_tokens = filter(lambda x: x not in unwanted, clean_tokens)\n",
    "        doc_to_tokens[document_id] = list(clean_tokens)\n",
    "    return doc_to_tokens\n",
    "\n",
    "#dividing into tokens - bigrams\n",
    "def bigramise_and_clean(corpus):\n",
    "    doc_to_bigrams = {}\n",
    "    for document_id in tqdm(corpus.fileids()):\n",
    "        words = corpus.words(document_id)\n",
    "        clean_words = [get_rid_of_underscores_and_dots_and_upper(w) for w in words]\n",
    "        clean_words = filter(lambda x: x not in unwanted, clean_words)\n",
    "        doc_to_bigrams[document_id] = list(nltk.bigrams(clean_words))\n",
    "    return doc_to_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method for indexing\n",
    "from tqdm import tqdm\n",
    "def index_files(index_filename, documents_to_tokens):\n",
    "    for document_id in tqdm(documents_to_tokens.keys()):\n",
    "        frequency_of_all_tokens = Counter(documents_to_tokens[document_id])\n",
    "        document_len = len(documents_to_tokens[document_id])\n",
    "        for token in documents_to_tokens[document_id]:\n",
    "            term_frequency_per_document = (frequency_of_all_tokens[token])/(document_len)\n",
    "            if token not in index_filename:\n",
    "                index_filename[token] = []\n",
    "            if (document_id, term_frequency_per_document) not in index_filename[token]:    \n",
    "                index_filename[token].append((document_id, term_frequency_per_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes index in form {'keyword': (document_id, term_frequency)} and n_documents as number of all documents in the corpus\n",
    "def rank_tf_idf_uni(ranked_index, index, n_document):\n",
    "    for word in tqdm(index):\n",
    "        docs_with_word = len(index[word])\n",
    "        ranked_index[word] = []\n",
    "        ranked_index.sync()\n",
    "        for pair in index[word]:\n",
    "            doc_id = pair[0]\n",
    "            doc_freq = 1+math.log(n_document/docs_with_word, 2)\n",
    "            rank = (doc_id, doc_freq*pair[1])\n",
    "            ranked_index[word].append(rank)\n",
    "            ranked_index.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_tf_idf_bi(ranked_index, index, n_document):\n",
    "    for bigram in tqdm(index):\n",
    "        docs_with_word = len(index[bigram])\n",
    "        word = \" \".join(list(bigram))\n",
    "        ranked_index[word] = []\n",
    "        ranked_index.sync()\n",
    "        for pair in index[bigram]:\n",
    "            doc_id = pair[0]\n",
    "            doc_freq = 1+math.log(n_document/docs_with_word, 2)\n",
    "            rank = (doc_id, doc_freq*pair[1])\n",
    "            ranked_index[word].append(rank)\n",
    "            ranked_index.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes query_tfidfs as {'keyword': tfidf} and doc_per_word as {'keyword': tfidf} in one document\n",
    "def cosine_similarity(query_tfidfs, tfidf_per_word):\n",
    "    query_sum = 0\n",
    "    doc_sum = 0\n",
    "    dot_product = 0\n",
    "    for query_word in query_tfidfs:\n",
    "        query_sum += (query_tfidfs[query_word]**2)\n",
    "        doc_sum += (tfidf_per_word[query_word]**2)\n",
    "        dot_product += (query_tfidfs[query_word] * tfidf_per_word[query_word])\n",
    "    query_norm = math.sqrt(query_sum)\n",
    "    doc_norm = math.sqrt(doc_sum)\n",
    "    if query_norm * doc_norm == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return dot_product/(query_norm * doc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf_for_document_id(document_id, subset_from_index):\n",
    "    tfidfs_per_word = {}\n",
    "    for word in subset_from_index:\n",
    "        for pair in subset_from_index[word]:\n",
    "            if pair[0] == document_id:\n",
    "                tfidfs_per_word[word] = pair[1]\n",
    "        #in case some query words are not present in the document\n",
    "        if word not in tfidfs_per_word:\n",
    "            tfidfs_per_word[word] = 0\n",
    "    \n",
    "    return tfidfs_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes result as a tuple of unigram_index search and bigram_index search, both as [(document_id, rank), ..]\n",
    "# prints example sentences for each document where query terms are present\n",
    "def find_example(results, query_unigrams, query_bigrams, corpus):\n",
    "    substring_window = 20 #to show the query with this many symbols on both sides\n",
    "    top_uni = results[0]\n",
    "    top_bi = results[1]\n",
    "    result_counter = 0 #number of results already shown\n",
    "    printed_docs = [] #to store docs we've already shown\n",
    "    \n",
    "    for pair in top_bi:\n",
    "        if pair[1] > 0 and result_counter < 8:\n",
    "            print(\"\\033[1m\"+pair[0].upper()+\"\\033[0m\")\n",
    "            result_counter += 1\n",
    "            printed_docs.append(pair[0])\n",
    "            for bigram in query_bigrams:\n",
    "                try:\n",
    "                    bg = \" \".join(list(bigram))\n",
    "                    occurence_index = corpus.raw(pair[0]).lower().index(bg)\n",
    "                    if occurence_index < substring_window:\n",
    "                        print(corpus.raw(pair[0])[:occurence_index+substring_window]+\"...\\n\")\n",
    "                    else:\n",
    "                        print(\"...\"+corpus.raw(pair[0])[occurence_index-substring_window:occurence_index+substring_window]+\"...\\n\")\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    #if there aren't enough results, we can go to the unigram index and supply more documents with parts of the query\n",
    "    # '4' is an arbitrary number, we continue until we've got 8 documents\n",
    "    if result_counter < 4:\n",
    "        for pair in top_uni:\n",
    "            if pair[1] > 0 and pair[0] not in printed_docs and result_counter < 8:\n",
    "                print(\"\\033[1m\"+pair[0].upper()+\"\\033[0m\")\n",
    "                result_counter += 1\n",
    "                printed_docs.append(pair[0])\n",
    "                for unigram in query_unigrams:\n",
    "                    try:\n",
    "                        occurence_index = corpus.raw(pair[0]).lower().index(unigram)\n",
    "                        if occurence_index < substring_window:\n",
    "                            print(corpus.raw(pair[0])[:occurence_index+substring_window]+\"...\\n\")\n",
    "                        else:\n",
    "                            print(\"...\"+corpus.raw(pair[0])[occurence_index-substring_window:occurence_index+substring_window]+\"...\\n\")\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method for searching: matching query and corpus documents\n",
    "def search(ranked, ranked_bigrams, query, corpus):\n",
    "    #dividing query into unigrams and bigrams\n",
    "    q_tokens = nltk.word_tokenize(query)\n",
    "    clean_q_tokens = [get_rid_of_underscores_and_dots_and_upper(t) for t in q_tokens]\n",
    "    bg_clean_q_tokens = clean_q_tokens\n",
    "    \n",
    "    clean_q_tokens = filter(lambda x: x not in unwanted, clean_q_tokens)\n",
    "    bg_clean_q_tokens = filter(lambda x: x not in unwanted, bg_clean_q_tokens)\n",
    "    \n",
    "    query_bigrams = list(nltk.bigrams(bg_clean_q_tokens))\n",
    "    \n",
    "    list_of_tokens = list(clean_q_tokens)\n",
    "    query_length = len(list_of_tokens)\n",
    "    \n",
    "    query_bg_length = len(query_bigrams)\n",
    "    \n",
    "    #calculating tf-idfs for query terms in unigrams\n",
    "    frequencies = Counter(list_of_tokens)\n",
    "    frequency_of_query_terms = {}\n",
    "    ranked_docs_per_term = {}\n",
    "    for term in frequencies:\n",
    "        if term in ranked:\n",
    "            frequency_of_query_terms[term] = frequencies[term]/query_length\n",
    "            term_idf = 1+math.log(n_documents/len(ranked[term]), 2)\n",
    "            frequency_of_query_terms[term] *= term_idf\n",
    "            ranked_docs_per_term[term] = ranked[term]\n",
    "            \n",
    "    #calculating tf-idfs for query bigrams\n",
    "    bg_frequencies = Counter(query_bigrams)\n",
    "    bg_frequency_of_query_terms = {}\n",
    "    bg_ranked_docs_per_term = {}\n",
    "    for pair in bg_frequencies:\n",
    "        term = \" \".join(list(pair))\n",
    "        if term in ranked_bigrams:\n",
    "            bg_frequency_of_query_terms[term] = bg_frequencies[pair]/query_bg_length\n",
    "            term_idf = 1+math.log(n_documents/len(ranked_bigrams[term]), 2)\n",
    "            bg_frequency_of_query_terms[term] *= term_idf\n",
    "            bg_ranked_docs_per_term[term] = ranked_bigrams[term]\n",
    "    \n",
    "    #if no query words exist in our indexes\n",
    "    if len(ranked_docs_per_term) == 0 and len(bg_ranked_docs_per_term) == 0:\n",
    "        print(\"Please try another key phrase, we can't seem to find this one, sorry...\\n\")\n",
    "        return\n",
    "    \n",
    "    #comparing similarities of query and docs vectors\n",
    "    ranked_docs_per_query = {}\n",
    "    bg_ranked_docs_per_query = {}\n",
    "    for document_id in corpus.fileids():\n",
    "        tfidfs_per_word = get_tfidf_for_document_id(document_id, ranked_docs_per_term)\n",
    "        bg_tfidfs_per_word = get_tfidf_for_document_id(document_id, bg_ranked_docs_per_term)\n",
    "        cs = cosine_similarity(frequency_of_query_terms, tfidfs_per_word)\n",
    "        bg_cs = cosine_similarity(bg_frequency_of_query_terms, bg_tfidfs_per_word)\n",
    "        if cs != 0 or bg_cs != 0:\n",
    "            ranked_docs_per_query[document_id] = cs\n",
    "            bg_ranked_docs_per_query[document_id] = bg_cs\n",
    "    \n",
    "    sorted_ranked_docs = sorted(ranked_docs_per_query.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    bg_sorted_ranked_docs = sorted(bg_ranked_docs_per_query.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    results = (sorted_ranked_docs, bg_sorted_ranked_docs)\n",
    "    find_example(results, list_of_tokens, query_bigrams, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm tokenising\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:29<00:00,  1.17s/it]\n",
      "100%|██████████| 18/18 [00:09<00:00,  2.58it/s]\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm indexing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:04<00:00,  4.94it/s]\n",
      "100%|██████████| 18/18 [00:08<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "'''# a cell that was used to prepare the data\n",
    "\n",
    "# preparing data\n",
    "print(\"I'm tokenising\")\n",
    "docs_to_tokens = tokenise_and_clean(corpus)\n",
    "docs_to_bigrams = bigramise_and_clean(corpus)\n",
    "\n",
    "# creating two indexes: unigram and bigram\n",
    "print(\"I'm indexing\")\n",
    "index_dictionary = {}\n",
    "index_bigrams = {}\n",
    "index_files(index_dictionary, docs_to_tokens)\n",
    "index_files(index_bigrams, docs_to_bigrams)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 142/52086 [00:00<00:36, 1408.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ranking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52086/52086 [00:05<00:00, 9986.75it/s] \n",
      "100%|██████████| 608635/608635 [00:30<00:00, 20157.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "'''# calculating TF-IDF for our two indexes\n",
    "print(\"I'm ranking\")\n",
    "with shelve.open('eng_index_uni',writeback=True) as ranked:\n",
    "    ranked.sync()\n",
    "with shelve.open('eng_index_bi',writeback=True) as ranked_bigrams:\n",
    "    ranked_bigrams.sync()\n",
    "ranked = shelve.open(\"eng_index_uni\", writeback=True)\n",
    "rank_tf_idf_uni(ranked, index_dictionary, n_documents)\n",
    "ranked_bigrams = shelve.open(\"eng_index_bi\", writeback=True)\n",
    "rank_tf_idf_bi(ranked_bigrams, index_bigrams, n_documents)\n",
    "print(\"Done!\")\n",
    "ranked.close()\n",
    "ranked_bigrams.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to search for?\n",
      "Moby Dick is a great novel by a great artist\n",
      "\u001b[1mMELVILLE-MOBY_DICK.TXT\u001b[0m\n",
      "[Moby Dick by Herman ...\n",
      "\n",
      ".... and Dan. HVAL.  This animal is named f...\n",
      "\n",
      "...e Lord had prepared a great fish to swal...\n",
      "\n",
      "...OLOGY.\n",
      "\n",
      "(Supplied by a Late Consumptiv...\n",
      "\n",
      "...e Lord had prepared a great fish to swal...\n",
      "\n",
      "\u001b[1mBRYANT-STORIES.TXT\u001b[0m\n",
      "...ed and indeed there is another lion! And...\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Once there was a great big jungle; ...\n",
      "\n",
      "...ttle Tulip said.\n",
      "\n",
      "By and by she heard ...\n",
      "\n",
      "...\n",
      "\n",
      "\n",
      "Once there was a great big jungle; ...\n",
      "\n",
      "\u001b[1mAUSTEN-PERSUASION.TXT\u001b[0m\n",
      "... one fair claim on his attachment;\n",
      "since...\n",
      "\n",
      "...ension, entitled to a great deal\n",
      "of comp...\n",
      "\n",
      "...ter had improved it by adding, for the i...\n",
      "\n",
      "...ension, entitled to a great deal\n",
      "of comp...\n",
      "\n",
      "\u001b[1mEDGEWORTH-PARENTS.TXT\u001b[0m\n",
      "...ssmore, in Ireland, is a small cabin,\n",
      "i...\n",
      "\n",
      "...by which she earned a great deal\n",
      "more t...\n",
      "\n",
      "...ce, all was managed by a Mr.\n",
      "Hopkins, a...\n",
      "\n",
      "...by which she earned a great deal\n",
      "more t...\n",
      "\n",
      "\u001b[1mAUSTEN-SENSE.TXT\u001b[0m\n",
      "...omfortably spent.  His attachment to the...\n",
      "\n",
      "...e his own,\n",
      "produced a great alteration i...\n",
      "\n",
      "... to his existence.\n",
      "\n",
      "By a former marriage...\n",
      "\n",
      "...e his own,\n",
      "produced a great alteration i...\n",
      "\n",
      "\u001b[1mCARROLL-ALICE.TXT\u001b[0m\n",
      "...know better'; and this Alice would not a...\n",
      "\n",
      "...be able! I shall be a great deal too far...\n",
      "\n",
      "...l, which was lit up by a row of lamps ha...\n",
      "\n",
      "...be able! I shall be a great deal too far...\n",
      "\n",
      "\u001b[1mAUSTEN-EMMA.TXT\u001b[0m\n",
      "...s of his heart\n",
      "and his amiable temper, h...\n",
      "\n",
      "...and would have been a great deal\n",
      "happier...\n",
      "\n",
      "...e had been supplied\n",
      "by an excellent woma...\n",
      "\n",
      "...and would have been a great deal\n",
      "happier...\n",
      "\n",
      "\u001b[1mCHESTERTON-BROWN.TXT\u001b[0m\n",
      "...nto an omnibus.  It is a rich confusion ...\n",
      "\n",
      "...I dare say there is a great deal\n",
      "to be s...\n",
      "\n",
      "...re ruled throughout by a terrible tidine...\n",
      "\n",
      "...I dare say there is a great deal\n",
      "to be s...\n",
      "\n",
      "... taken up Art, or a great artist whom\n",
      "th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# actual searching\n",
    "ranked = shelve.open(\"eng_index_uni\", writeback=True)\n",
    "ranked_bigrams = shelve.open(\"eng_index_bi\", writeback=True)\n",
    "query = input(\"What would you like to search for?\\n\")        \n",
    "search(ranked, ranked_bigrams, query, corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
