{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/Sonya/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, SimpleRNN\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Baseline and Modifications For IMDB data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "data = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[0][0]\n",
    "y = data[0][1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.2, \n",
    "                                                    random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 376,405\n",
      "Trainable params: 376,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 309s - loss: 0.5685 - acc: 0.6957   \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 307s - loss: 0.2771 - acc: 0.8868   \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 339s - loss: 0.2133 - acc: 0.9198   \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 318s - loss: 0.1894 - acc: 0.9316   \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 315s - loss: 0.1496 - acc: 0.9471   \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 304s - loss: 0.1274 - acc: 0.9553   \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 321s - loss: 0.1141 - acc: 0.9616   \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 306s - loss: 0.0988 - acc: 0.9682   \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 298s - loss: 0.0817 - acc: 0.9745   \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 306s - loss: 0.0665 - acc: 0.9806   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122a01518>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(2*top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.34%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Movie Reviews as Test Set and IMDB test\n",
    "Here I decided to test the LSTM on another data set of movie reviews, to see how well the model generalises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "                for category in movie_reviews.categories()\n",
    "                for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "import random\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting rid of stopwords\n",
    "unwanted = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "unwanted.update(list('!\"#$%&\\'()*+,-./:;<=>? @[\\\\]^_`{|}~Â£'))\n",
    "unwanted.update(['also', 'even', '--'])\n",
    "words = movie_reviews.words()\n",
    "words = filter(lambda x: x not in unwanted, words)\n",
    "\n",
    "#calculating frequency dictionary for all words\n",
    "all_words = nltk.FreqDist(w.lower() for w in words)\n",
    "most_frequent = all_words.most_common(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tuples to dictionary\n",
    "def tuple_to_dict(some_list):\n",
    "    new_list = {}\n",
    "    for pair in some_list:\n",
    "        new_list[pair[0]] = pair[1]\n",
    "    return new_list\n",
    "\n",
    "vocabulary = list(tuple_to_dict(most_frequent).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nltk = []\n",
    "Y_nltk = []\n",
    "for d in documents:\n",
    "    Y_nltk.append(d[1])\n",
    "    words_to_ints = []\n",
    "    for word in d[0]:\n",
    "        if word in vocabulary:\n",
    "            words_to_ints.append(vocabulary.index(word))\n",
    "        else:\n",
    "            words_to_ints.append(0)\n",
    "    X_nltk.append(words_to_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "Y_nltk = lb.fit_transform(Y_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_nltk = sequence.pad_sequences(X_nltk, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.85%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_nltk, Y_nltk, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model doesn't really generalise very well. What I understood after I've done all of this is that the frequencies of the words are different - even if the words are the same since they are calculated at different times - so unless I calculate frequencies of all the words collectively, it won't work as intended. So this is where I'm calculating the frequencies of IMDB and NLTK movie reviews collectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "neg_files_train=glob.glob(\"./aclImdb/train/neg/*.txt\")\n",
    "neg_files_test=glob.glob(\"./aclImdb/test/neg/*.txt\")\n",
    "pos_files_train=glob.glob(\"./aclImdb/train/pos/*.txt\")\n",
    "pos_files_test=glob.glob(\"./aclImdb/test/pos/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_and_read(some_list):\n",
    "    docs = []\n",
    "    for file in some_list:\n",
    "        f = open(file, \"r\")\n",
    "        text = f.readlines()\n",
    "        docs.append(nltk.word_tokenize(text[0]))\n",
    "        f.close()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_docs_tr = open_and_read(neg_files_train)\n",
    "neg_docs_test = open_and_read(neg_files_test)\n",
    "pos_docs_tr = open_and_read(pos_files_train)\n",
    "pos_docs_test = open_and_read(pos_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating frequencies for the whole text\n",
    "def iterate_and_append(lists):\n",
    "    all_words = []\n",
    "    for l in lists:\n",
    "        for doc in l:\n",
    "            for word in doc:\n",
    "                all_words.append(word.lower())\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = iterate_and_append([neg_docs_tr, neg_docs_test, pos_docs_tr, pos_docs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text += movie_reviews.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unwanted = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "unwanted.update(list('!\"#$%&\\'()*+,-./:;<=>? @[\\\\]^_`{|}~Â£1234567890abcdefghijklmnopqrstuvwxyz'))\n",
    "unwanted.update(['also', 'even', '--', 'film', 'director', 'character',\"'ve\", \"'m\", \"'d\",\"n't\",'``',\"''\",\"'s\",\"'re\",\"'ll\",'...','br','movie','one'])\n",
    "\n",
    "all_text = filter(lambda x: x not in unwanted, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculating frequency dictionary for all words\n",
    "words_from_everywhere = nltk.FreqDist(all_text)\n",
    "most_frequent_words = words_from_everywhere.most_common(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfw = tuple_to_dict(most_frequent_words)\n",
    "vocabulary = list(mfw.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def transform_words_into_ints(list_of_docs):\n",
    "    x = []\n",
    "    for doc in tqdm(list_of_docs):\n",
    "        words_to_ints=[]\n",
    "        for word in doc:\n",
    "            if word in vocabulary:\n",
    "                words_to_ints.append(vocabulary.index(word))\n",
    "            else:\n",
    "                words_to_ints.append(0)\n",
    "        x.append(words_to_ints)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 12500/12500 [07:41<00:00, 27.08it/s]\n",
      "100%|ââââââââââ| 12500/12500 [07:52<00:00, 26.45it/s]\n"
     ]
    }
   ],
   "source": [
    "X_all_train = transform_words_into_ints(neg_docs_tr) + transform_words_into_ints(pos_docs_tr)\n",
    "Y_all_train = numpy.concatenate((numpy.zeros((12500,), dtype=numpy.int), numpy.ones((12500,), dtype=numpy.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 12500/12500 [07:06<00:00, 22.21it/s]\n",
      "100%|ââââââââââ| 12500/12500 [06:52<00:00, 30.34it/s]\n"
     ]
    }
   ],
   "source": [
    "X_all_test = transform_words_into_ints(neg_docs_test) + transform_words_into_ints(pos_docs_test)\n",
    "Y_all_test = numpy.concatenate((numpy.zeros((12500,), dtype=numpy.int), numpy.ones((12500,), dtype=numpy.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 2000/2000 [00:01<00:00, 1678.72it/s]\n"
     ]
    }
   ],
   "source": [
    "nltk_test = []\n",
    "for doc in tqdm(documents):\n",
    "    word_to_ints=[]\n",
    "    for word in doc:\n",
    "        if word in vocabulary:\n",
    "            word_to_ints.append(vocabulary.index(word))\n",
    "        else:\n",
    "            word_to_ints.append(0)\n",
    "    nltk_test.append(word_to_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all_train += nltk_test[:1000]\n",
    "Y_train_all = numpy.append(Y_all_train, Y_nltk[:1000])\n",
    "Y_test_all = numpy.append(Y_all_test, Y_nltk[1000:])\n",
    "X_all_test += nltk_test[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all_train = sequence.pad_sequences(X_all_train, maxlen=300)\n",
    "X_all_test = sequence.pad_sequences(X_all_test, maxlen=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the same network architecture from above for this task - except that this model will be called __model_nltk_plus__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 300, 64)           384000    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 300, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 443,477\n",
      "Trainable params: 443,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      "26000/26000 [==============================] - 133s - loss: 0.4282 - acc: 0.7819   \n",
      "Epoch 2/6\n",
      "26000/26000 [==============================] - 117s - loss: 0.2710 - acc: 0.8843   \n",
      "Epoch 3/6\n",
      "26000/26000 [==============================] - 120s - loss: 0.2294 - acc: 0.9023   \n",
      "Epoch 4/6\n",
      "26000/26000 [==============================] - 120s - loss: 0.1965 - acc: 0.9185   \n",
      "Epoch 5/6\n",
      "26000/26000 [==============================] - 127s - loss: 0.1675 - acc: 0.9282   \n",
      "Epoch 6/6\n",
      "26000/26000 [==============================] - 125s - loss: 0.1389 - acc: 0.9405   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c49d10b38>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 64\n",
    "model_nltk_plus = Sequential()\n",
    "model_nltk_plus.add(Embedding(6000, embedding_vector_length, input_length=300))\n",
    "model_nltk_plus.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_nltk_plus.add(MaxPooling1D(pool_size=6))\n",
    "model_nltk_plus.add(Dropout(0.2))\n",
    "model_nltk_plus.add(LSTM(100))\n",
    "model_nltk_plus.add(Dropout(0.2))\n",
    "model_nltk_plus.add(Dense(1, activation='sigmoid'))\n",
    "model_nltk_plus.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_nltk_plus.summary())\n",
    "model_nltk_plus.fit(X_all_train, Y_train_all, epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.22%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model_nltk_plus.evaluate(X_all_test, Y_test_all, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classification For Tweets in Russian\n",
    "The tweets are taken from here: http://study.mokoron.com/.\n",
    "The tweets there were classified with the help of an automatic system into positive and negative ones. I'm going to use them to see how LSTM will classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pos = pd.read_csv(\"./twitter_russian/positive.csv\",sep=';')\n",
    "X_neg = pd.read_csv(\"./twitter_russian/negative.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tdate</th>\n",
       "      <th>Tname</th>\n",
       "      <th>Ttext</th>\n",
       "      <th>Ttype</th>\n",
       "      <th>Trep</th>\n",
       "      <th>Tfav</th>\n",
       "      <th>Tstcount</th>\n",
       "      <th>Tfol</th>\n",
       "      <th>Tfrlen</th>\n",
       "      <th>ListCount</th>\n",
       "      <th>na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>408906762813579264</td>\n",
       "      <td>1386325944</td>\n",
       "      <td>dugarchikbellko</td>\n",
       "      <td>Ð½Ð° ÑÐ°Ð±Ð¾ÑÐµ Ð±ÑÐ» Ð¿Ð¾Ð»Ð½ÑÐ¹ Ð¿Ð¸Ð´Ð´ÐµÑ :| Ð¸ ÑÐ°Ðº ÐºÐ°Ð¶Ð´Ð¾Ðµ Ð·Ð°...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8064</td>\n",
       "      <td>111</td>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>408906818262687744</td>\n",
       "      <td>1386325957</td>\n",
       "      <td>nugemycejela</td>\n",
       "      <td>ÐÐ¾Ð»Ð»ÐµÐ³Ð¸ ÑÐ¸Ð´ÑÑ ÑÑÐ±ÑÑÑÑ Ð² Urban terror, Ð° Ñ Ð¸Ð·-Ð·...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>408906858515398656</td>\n",
       "      <td>1386325966</td>\n",
       "      <td>4post21</td>\n",
       "      <td>@elina_4post ÐºÐ°Ðº Ð³Ð¾Ð²Ð¾ÑÑÑ Ð¾Ð±ÐµÑÐ°Ð½Ð¾Ð³Ð¾ ÑÑÐ¸ Ð³Ð¾Ð´Ð° Ð¶Ð´...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>718</td>\n",
       "      <td>49</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>408906914437685248</td>\n",
       "      <td>1386325980</td>\n",
       "      <td>Poliwake</td>\n",
       "      <td>ÐÐµÐ»Ð°Ñ ÑÐ¾ÑÐ¾ÑÐµÐ³Ð¾ Ð¿Ð¾Ð»ÑÑÐ° Ð¸ ÑÐ´Ð°ÑÐ½Ð¾Ð¹ Ð¿Ð¾ÑÐ°Ð´ÐºÐ¸,Ñ Ð±ÑÐ´Ñ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10628</td>\n",
       "      <td>207</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>408906914723295232</td>\n",
       "      <td>1386325980</td>\n",
       "      <td>capyvixowe</td>\n",
       "      <td>ÐÐ±Ð½Ð¾Ð²Ð¸Ð» Ð·Ð° ÐºÐ°ÐºÐ¸Ð¼-ÑÐ¾ Ð»ÐµÑÐ¸Ð¼ surf, ÑÐµÐ¿ÐµÑÑ Ð½Ðµ ÑÐ°Ð±Ð¾...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id       Tdate            Tname  \\\n",
       "0  408906762813579264  1386325944  dugarchikbellko   \n",
       "1  408906818262687744  1386325957     nugemycejela   \n",
       "2  408906858515398656  1386325966          4post21   \n",
       "3  408906914437685248  1386325980         Poliwake   \n",
       "4  408906914723295232  1386325980       capyvixowe   \n",
       "\n",
       "                                               Ttext  Ttype  Trep  Tfav  \\\n",
       "0  Ð½Ð° ÑÐ°Ð±Ð¾ÑÐµ Ð±ÑÐ» Ð¿Ð¾Ð»Ð½ÑÐ¹ Ð¿Ð¸Ð´Ð´ÐµÑ :| Ð¸ ÑÐ°Ðº ÐºÐ°Ð¶Ð´Ð¾Ðµ Ð·Ð°...     -1     0     0   \n",
       "1  ÐÐ¾Ð»Ð»ÐµÐ³Ð¸ ÑÐ¸Ð´ÑÑ ÑÑÐ±ÑÑÑÑ Ð² Urban terror, Ð° Ñ Ð¸Ð·-Ð·...     -1     0     0   \n",
       "2  @elina_4post ÐºÐ°Ðº Ð³Ð¾Ð²Ð¾ÑÑÑ Ð¾Ð±ÐµÑÐ°Ð½Ð¾Ð³Ð¾ ÑÑÐ¸ Ð³Ð¾Ð´Ð° Ð¶Ð´...     -1     0     0   \n",
       "3  ÐÐµÐ»Ð°Ñ ÑÐ¾ÑÐ¾ÑÐµÐ³Ð¾ Ð¿Ð¾Ð»ÑÑÐ° Ð¸ ÑÐ´Ð°ÑÐ½Ð¾Ð¹ Ð¿Ð¾ÑÐ°Ð´ÐºÐ¸,Ñ Ð±ÑÐ´Ñ...     -1     0     0   \n",
       "4  ÐÐ±Ð½Ð¾Ð²Ð¸Ð» Ð·Ð° ÐºÐ°ÐºÐ¸Ð¼-ÑÐ¾ Ð»ÐµÑÐ¸Ð¼ surf, ÑÐµÐ¿ÐµÑÑ Ð½Ðµ ÑÐ°Ð±Ð¾...     -1     0     0   \n",
       "\n",
       "   Tstcount   Tfol  Tfrlen  ListCount  na  \n",
       "0         0   8064     111         94   2  \n",
       "1         0     26      42         39   0  \n",
       "2         0    718      49        249   0  \n",
       "3         0  10628     207        200   0  \n",
       "4         0     35      17         34   0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sonya/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([X_pos,X_neg])\n",
    "X_rus = data['Ttext']\n",
    "y_rus = data['Ttype']\n",
    "y_rus[y_rus==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unwanted_rus = set(nltk.corpus.stopwords.words(\"russian\"))\n",
    "unwanted_rus.update(list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~Â£1234567890tÑÑÑÐ¾'))\n",
    "unwanted_rus.update([\"ff\", \"ru\", '--', \"to\", '``',\"''\",\"...\",\"co\", \"rt\",\"http\",'lt',u'Ñ',u'Ð¸',u'ÑÐ¾',u'Ð½Ð¸Ð±ÑÐ´Ñ','â¦'])\n",
    "\n",
    "all_words = []\n",
    "X_rus_tokenised = []\n",
    "for tweet in X_rus:\n",
    "    words = nltk.word_tokenize(tweet)\n",
    "    cur_words = [w.lower() for w in words]\n",
    "    cur_words = list(filter(lambda x: x not in unwanted_rus, cur_words))\n",
    "    X_rus_tokenised.append(cur_words)\n",
    "    all_words += cur_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = nltk.FreqDist(all_words)\n",
    "most_com = list(tuple_to_dict(freq_words.most_common(10000)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_rus = []\n",
    "for tweet in X_rus_tokenised:\n",
    "    words_to_ints = []\n",
    "    for word in tweet:\n",
    "        if word in most_com:\n",
    "            words_to_ints.append(most_com.index(word))\n",
    "        else:\n",
    "            words_to_ints.append(0)\n",
    "    encoded_rus.append(words_to_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_rus_train, X_rus_test, y_rus_train, y_rus_test = train_test_split(encoded_rus, y_rus, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_rus_train = sequence.pad_sequences(X_rus_train, maxlen=50)\n",
    "X_rus_test = sequence.pad_sequences(X_rus_test, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 32)            192000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 50, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 248,405\n",
      "Trainable params: 248,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 181467 samples, validate on 20000 samples\n",
      "Epoch 1/3\n",
      "181467/181467 [==============================] - 138s - loss: 0.5182 - acc: 0.7224 - val_loss: 0.4972 - val_acc: 0.7398\n",
      "Epoch 2/3\n",
      "181467/181467 [==============================] - 125s - loss: 0.4680 - acc: 0.7626 - val_loss: 0.4911 - val_acc: 0.7490\n",
      "Epoch 3/3\n",
      "181467/181467 [==============================] - 131s - loss: 0.4314 - acc: 0.7872 - val_loss: 0.5016 - val_acc: 0.7464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c4d12ee48>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "model_rus = Sequential()\n",
    "model_rus.add(Embedding(6000, embedding_vector_length, input_length=50))\n",
    "model_rus.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model_rus.add(MaxPooling1D(pool_size=5))\n",
    "#model_rus.add(Dropout(0.2))\n",
    "model_rus.add(LSTM(100, unit_forget_bias=True))\n",
    "model_rus.add(Dropout(0.2))\n",
    "model_rus.add(Dense(1, activation='sigmoid'))\n",
    "model_rus.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rus.summary())\n",
    "model_rus.fit(X_rus_train, y_rus_train, validation_data=(X_rus_test[:20000], y_rus_test[:20000]), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45280/45367 [============================>.] - ETA: 0s\n",
      "Accuracy: 74.74%\n"
     ]
    }
   ],
   "source": [
    "scores = model_rus.evaluate(X_rus_test, y_rus_test)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Characters Instead of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(\" \".join(most_com))))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_encoded = []\n",
    "for tweet in X_rus_tokenised:\n",
    "    encoded_tweet = []\n",
    "    for word in tweet:\n",
    "        if word in most_com:\n",
    "            encoded_tweet += [char_to_int[l] for l in word]\n",
    "        else:\n",
    "            encoded_tweet += [0 for l in word]\n",
    "    char_encoded.append(encoded_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_encoded = sequence.pad_sequences(char_encoded, maxlen=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_char_train, x_char_test, y_char_train, y_char_test = train_test_split(char_encoded, y_rus, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the model\n",
    "embedding_vector_length = 16\n",
    "model_rus = Sequential()\n",
    "model_rus.add(Embedding(100, embedding_vector_length, input_length=140))\n",
    "model_rus.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "model_rus.add(MaxPooling1D(pool_size=5))\n",
    "#model_rus.add(Dropout(0.2))\n",
    "model_rus.add(LSTM(80, unit_forget_bias=True))\n",
    "model_rus.add(Dropout(0.2))\n",
    "model_rus.add(Dense(1, activation='sigmoid'))\n",
    "model_rus.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rus.summary())\n",
    "model_rus.fit(x_char_train, y_char_train, validation_data=(x_char_test[:10000], y_char_test[:10000]), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45367/45367 [==============================] - 38s    \n",
      "\n",
      "Accuracy: 69.68%\n"
     ]
    }
   ],
   "source": [
    "scores = model_rus.evaluate(x_char_test, y_char_test)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
